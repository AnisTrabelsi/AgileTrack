# DevOpsTrack

DevOpsTrack est une **plateforme microâ€‘services** pour suivre des pipelines CI/CD, gÃ©rer des projets techniques et agrÃ©ger des mÃ©triques dâ€™exÃ©cution en temps rÃ©el.

---

## ğŸš© FonctionnalitÃ©s clÃ©s

* **Authentification JWT**Â : connexion sÃ©curisÃ©e, rafraÃ®chissement de jetons.
* **Gestion des utilisateurs**Â : rÃ´les & droits dans PostgreSQL.
* **Module Projets**Â : CRUD dÃ©pÃ´ts / environnements / versions (FastAPIÂ +Â MongoDB).
* **Module TÃ¢ches**Â : file Redis simulant des jobs CI/CD, Ã©tat en temps rÃ©el (APIÂ +Â worker).
* **MÃ©triques & Logs**Â : exposition `/metrics` (Prometheus), stockage InfluxDB.
* **Tableau de bord Web**Â : ReactÂ 18 (Vite) + Tailwind (graphique builds, Ã©tat jobs).
* **Registry dâ€™images**Â : **GHCR** (par dÃ©faut). *NexusÂ 3 reste optionnel en local via Compose.*
* **Surveillance**Â : Prometheus scrappe les services, Grafana propose des dashboards.
* **Pipeline CI/CD**Â : GitHub Actions â†’ build â†’ push GHCR â†’ dÃ©ploiement (TerraformÂ +Â kubectl).

---

## âš™ï¸ Pile technologique

| Couche           | Outils principaux                                                   |
| ---------------- | ------------------------------------------------------------------- |
| Frontend         | ReactÂ 18 Â· Vite Â· TailwindCSS                                       |
| Services         | DjangoÂ (Auth) Â· FastAPIÂ (Projects) Â· Node.jsÂ (Tasks) Â· GoÂ (Metrics) |
| Bases de donnÃ©es | PostgreSQL Â· MongoDB Â· Redis Â· InfluxDB                             |
| Conteneurs       | Docker Â· Docker Compose                                             |
| Orchestration    | Kubernetes (k3d en local, **EKS** en prod)                          |
| Registry         | **GHCR** Â· *(NexusÂ 3 optionnel en local)*                           |
| CI/CD            | Git & GitHub Actions (SonarCloud + BuildÂ &Â Push)                    |
| IaC              | Terraform Â· Ansible (module `kubernetes.core.k8s`)                  |
| Monitoring       | Prometheus Â· Grafana                                                |

---

## ğŸš€ Lancer en **local** (DockerÂ Compose)

```bash
# Cloner le dÃ©pÃ´t
git clone https://github.com/<owner>/<repo>.git
cd <repo>

# Lancer les services (inclut Nexus & SonarQube si activÃ©s)
docker compose -f deploy/compose.yml up --build -d
```

**AccÃ¨s rapides** :

* Auth APIÂ : [http://localhost:8000](http://localhost:8000)
* Projects APIÂ : [http://localhost:8001](http://localhost:8001)
* Tasks APIÂ : [http://localhost:8002](http://localhost:8002)
* PrometheusÂ : [http://localhost:9090](http://localhost:9090)
* GrafanaÂ : [http://localhost:3000](http://localhost:3000) (admin / admin)
* *(Optionnel)*Â NexusÂ : [http://localhost:8081](http://localhost:8081)

> ArrÃªtÂ &Â nettoyageÂ : `docker compose -f deploy/compose.yml down -v`

---

## â˜¸ï¸ DÃ©ploiement **Kubernetes â€” local (k3d)**

<details>
<summary>Clique pour les Ã©tapes k3d</summary>

### 1) CrÃ©er un clusterÂ k3d

```bash
k3d cluster create devopstrack --servers 1 --agents 2 -p "80:80@loadbalancer"
kubectl config use-context k3d-devopstrack
kubectl get nodes
```

### 2) PrÃ©parer le namespace + pullÂ secret GHCR

> CrÃ©er un **PAT** GitHub avec scope **`read:packages`**

```bash
kubectl apply -f deploy/k8s/base/namespaces.yaml

kubectl -n devopstrack create secret docker-registry image-pull-ghcr \
  --docker-server=ghcr.io \
  --docker-username=<github-username> \
  --docker-password=<PAT> \
  --docker-email=<email>

kubectl -n devopstrack patch serviceaccount default --type merge \
  -p '{"imagePullSecrets":[{"name":"image-pull-ghcr"}]}'
```

### 3) Installer les bases (Helm)

```bash
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update

helm install auth-db bitnami/postgresql -n devopstrack \
  --set auth.username=postgres,auth.password=postgres,auth.database=auth

helm install projects-db bitnami/mongodb -n devopstrack --set auth.enabled=false

helm install tasks-redis bitnami/redis -n devopstrack --set auth.enabled=false

helm install metrics-db bitnami/influxdb2 -n devopstrack \
  --set adminUser.username=admin,adminUser.password=admin123,adminUser.token=dev-token,adminUser.organization=devopstrack,adminUser.bucket=metrics
```

### 4) DÃ©ployer les applications

```bash
kubectl apply -f deploy/k8s/base/apps/k8s-devopstrack-apps.yaml
```

</details>

---

## â˜ï¸ DÃ©ploiement **production â€”Â AWSÂ EKS**

### ğŸ”‘ PrÃ©requis AWS

| Ressource                             | Commentaire                                             |
| ------------------------------------- | ------------------------------------------------------- |
| AWSÂ Account                           | + IAM user/role avec droits administrateur              |
| Bucket S3Â `devopstrack-tfstate-<id>`  | Stockage Ã©tat Terraform                                 |
| Table DynamoDBÂ `devopstrack-tf-lock`  | Verrouillage Terraform                                  |
| RÃ´le IAM OIDC **`DevOpsTrackGitHub`** | TrustÂ Policy vers `token.actions.githubusercontent.com` |
| Secrets GitHubÂ `AWS_ROLE_TO_ASSUME`   | ARN du rÃ´le ciâ€‘dessus                                   |

### 1) Provisionner lâ€™infra (Terraform)

```bash
cd infra/terraform
terraform init
terraform plan
terraform apply -auto-approve

# Configure kubectl :
aws eks update-kubeconfig --name devopstrack-eks --region eu-west-3
```

### 2) CI/CD GitHub Actions

1. **`ci.yml`**Â : build, tests, push images GHCR.
2. **`infra-plan-apply.yml`**Â : plan sur PR ; apply sur `main`.
3. **`deploy-eks.yml`**Â : applique `deploy/k8s/base/all-in-one.yaml` dÃ¨s quâ€™une image est poussÃ©e.

### 3) DÃ©ployer manuellement (si besoin)

```bash
kubectl apply -f deploy/k8s/base/all-in-one.yaml
kubectl -n devopstrack get pods
```

> Le fichier `all-in-one.yaml` crÃ©e le namespace `devopstrack`, les Secrets, Deployments, Services et lâ€™IngressRoute Traefik ; il route `/projects`, `/tasks` et `/`.

---

## ğŸ”„ Pipeline **CI/CD**

| Ã‰tape                | Action GitHub          | Description                                   |
| -------------------- | ---------------------- | --------------------------------------------- |
| **QualitÃ©**          | `ci.yml` â†’ SonarCloud  | Analyse code et tests unitaires               |
| **Build & Push**     | `ci.yml`               | Image `${{ github.sha }}` + `latest` sur GHCR |
| **Plan/Apply Infra** | `infra-plan-apply.yml` | Terraform planÂ (PR) / applyÂ (main)            |
| **Deploy App**       | `deploy-eks.yml`       | `kubectl apply` du manifeste K8s              |

Secrets requisÂ : `SONAR_TOKEN`, `AWS_ROLE_TO_ASSUME`.

---

## ğŸ“‚ Arborescence

```
frontend/
auth-service/
projects-service/
tasks-service/
metrics-service/
deploy/
  compose.yml
  k8s/
    base/
      namespaces.yaml
      apps/
        k8s-devopstrack-apps.yaml
infra/
  terraform/
  ansible/
.github/
  workflows/
    ci.yml
    infra-plan-apply.yml
    deploy-eks.yml
```

---

## ğŸ§ª Tests rapides (EKS)

```bash
kubectl get nodes
kubectl -n devopstrack get pods
kubectl -n devopstrack port-forward svc/projects-service 8001:8001
curl http://127.0.0.1:8001/docs
```

---

## ğŸ—ï¸ Composants AWS mobilisÃ©s

| Couche               | Service AWS                      | RÃ´le prÃ©cis dans lâ€™architecture                                  |
| -------------------- | -------------------------------- | ---------------------------------------------------------------- |
| **RÃ©seau**           | VPC                              | Isolation rÃ©seau (CIDRÂ `10.0.0.0/16`).                           |
|                      | Subnets privÃ©s & publics         | 3Â AZ (euâ€‘westâ€‘3a/b/c). PrivÃ©sâ€¯: nÅ“udsâ€¯EKS. Publicsâ€¯: LB, NATÂ GW. |
|                      | InternetÂ Gateway                 | AccÃ¨s Internet subnets publics.                                  |
|                      | NATÂ Gateway                      | Sortie Internet des nÅ“uds privÃ©s.                                |
|                      | Security Groups                  | Pareâ€‘feu autour du controlâ€‘plane, des nÅ“uds et du LB.            |
| **Calcul**           | EKS (Elastic Kubernetes Service) | Cluster Kubernetes 1.30 managÃ©.                                  |
|                      | Managed Node Groups              | NÅ“uds EC2Â `t3.medium`, autoscalingÂ 2â€‘4.                          |
| **Stockage**         | S3                               | Bucket tfstate, Ã©ventuels backups.                               |
|                      | DynamoDB                         | Table `devopstrack-tf-lock` (lock state).                        |
|                      | ECR                              | Registre dâ€™images `frontend`, `auth-service`, â€¦                  |
| **SÃ©curitÃ©**         | IAM roles & policies             | RÃ´les cluster, nodes + OIDC GitHub.                              |
|                      | KMS                              | Chiffrement secrets Kubernetes.                                  |
| **ObservabilitÃ©**    | CloudWatch Logs & Metrics        | Logs controlâ€‘plane & ContainerÂ Insights.                         |
| **RÃ©seau app**       | Elastic Load Balancer            | ALB/NLB provisionnÃ© via Traefik.                                 |
| **DNS/TLS (option)** | RouteÂ 53 + ACM                   | Domaine & certificats pour Traefik.                              |

**Pourquoi ces choixâ€¯?**

1. VPC multiâ€‘AZÂ : haute dispo.
2. NATÂ GW uniqueÂ : coÃ»t â†” simplicitÃ©.
3. Managedâ€¯NodesÂ : patching & autoscaling gÃ©rÃ©s.
4. ECRÂ : latence min vers EKS.
5. S3Â +Â DynamoDB backendÂ : standard Terraform.
6. OIDC GitHubÂ : pas de clÃ©s statiques.
7. KMSÂ : chiffrement des secrets.
8. CloudWatchÂ : journaux & alertes intÃ©grÃ©s.
9. ELB automatisÃ© par Traefik.

---

## â— Bonnes pratiques

* Utiliser des bases **managÃ©es** en prod (RDS, Atlasâ€¦).
* Stocker les secrets dans **AWSÂ SecretsÂ Manager** avec *Externalâ€‘Secrets*.
* Ajouter des probes HTTP `/healthz` aux services.
* Activer HPA (ou KEDA) pour lâ€™autoâ€‘scaling.
